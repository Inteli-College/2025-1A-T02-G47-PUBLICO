 https://arxiv.org/pdf/1706.03762

Alguns benefÃ­cios de acordo com o artigo, fala que essa arquitetura se mostrou superior em quesitos de qualidade, ao mesmo tempo que Ã© mais paralelizÃ¡vel e requer significantemente menos tempo para [[treinar]].


- MÃ©trica **[[BLEU]]** ( Bilingual evaluation Understudy ) Ã© uma das mais comuns para avaliar a qualidade de traduÃ§Ãµes automÃ¡ticas. Ela mede o quÃ£o prÃ³xima a saÃ­da do modelo estÃ¡ de uma ou mais traduÃ§Ãµes de referÃªncia, usando n-gramas para comparaÃ§Ã£o. A mÃ©trica foi introduzida em 2002 por [[Kishore Papineni]] e sua equipe na IBM

- Na tarefa de traduÃ§Ã£o de inglÃªs para francÃªs, nosso modelo estabelece uma nova pontuaÃ§Ã£o BLEU de Ãºltima geraÃ§Ã£o de modelo Ãºnico de ==41,8 apÃ³s treinando por 3,5 dias em oito GPUs==


## ğŸ”¥ O que significa "Evitar a recorrÃªncia" ?

Antes do Transformer, os modelos de traduÃ§Ã£o automÃ¡tica mais avanÃ§ados usavam **Redes Neurais Recorrentes (RNNs)** ou **LSTMs (Long Short-Term Memory)**. Esses modelos processavam a entrada **sequencialmente**, ou seja, palavra por palavra, dependendo das informaÃ§Ãµes anteriores. Esse processo tinha algumas limitaÃ§Ãµes:

1. **Dificuldade em capturar relaÃ§Ãµes de longo prazo** â€” InformaÃ§Ãµes no inÃ­cio da sequÃªncia podiam ser esquecidas Ã  medida que o modelo processava mais palavras.
2. **Baixa paralelizaÃ§Ã£o** â€” Como cada palavra dependia das palavras anteriores, as RNNs nÃ£o podiam ser processadas em paralelo, tornando o treinamento mais lento.

O **Transformer evita esse problema eliminando a recorrÃªncia** e **processando todas as palavras da sequÃªncia ao mesmo tempo** por meio de ==**self-attention**==.


## ğŸ¤– Como o Transformer funciona?

O Transformer Ã© baseado em um **mecanismo de atenÃ§Ã£o**, que permite que cada palavra na entrada se conecte diretamente a todas as outras palavras na sequÃªncia, sem precisar percorrÃª-las de maneira sequencial.

### ğŸ”¹ Principais componentes:

1. **Self-Attention (AutoatenÃ§Ã£o)** âš¡
    - Cada palavra na entrada **olha para todas as outras palavras ao mesmo tempo**, calculando **quais palavras sÃ£o mais importantes** para determinar seu significado.
    - Assim, palavras podem estabelecer conexÃµes globais, mesmo que estejam **distantes na frase**.
    
2. **Mecanismo de atenÃ§Ã£o multi-head** ğŸ”
    - O Transformer nÃ£o usa apenas **uma Ãºnica "visÃ£o"** da entrada, mas **mÃºltiplas "cabeÃ§as de atenÃ§Ã£o"** para entender diferentes relaÃ§Ãµes entre palavras ao mesmo tempo.
    
3. **Positional Encoding** ğŸŒ€
    - Como o Transformer nÃ£o processa palavras sequencialmente, ele precisa de uma forma de saber a ordem das palavras na frase.
    - Ele adiciona uma **codificaÃ§Ã£o de posiÃ§Ã£o** (Positional Encoding) a cada palavra para manter a estrutura da frase.
    
4. **Camadas Feed-Forward e NormalizaÃ§Ã£o** ğŸ—ï¸
    - Depois da self-attention, hÃ¡ camadas totalmente conectadas (**feed-forward**) que ajudam a refinar as representaÃ§Ãµes aprendidas.
    
5. **ParalelizaÃ§Ã£o ğŸš€**
    - Como o Transformer processa todas as palavras **ao mesmo tempo**, ele pode usar muito mais **paralelizaÃ§Ã£o** (por exemplo, rodando em GPUs com eficiÃªncia), tornando o treinamento **mais rÃ¡pido** do que modelos baseados em recorrÃªncia.


## O que Ã© uma estrutura Encoder-Decoder ?

- Encoder ( Codificador ):
	- Recebe uma sequencia de entrada ( Por exemplo, uma frase em inglÃªs ).
	- Converte essa sequencia em **representaÃ§Ãµes contÃ­nuas** ( Vetores ).
	- Essas representaÃ§Ãµes sÃ£o chamadas de $z = (z1, ...., zn)$, e sÃ£o como um "resumo" do significado da entrada.

- Decoder ( Decodificador ):
	- Usa essa representaÃ§Ã£o **z** para gerar a **sequÃªncia de saÃ­da** ( Por exemplo, a traduÃ§Ã£o para alemÃ£o)
	- Ele produz a saÃ­da **palavra por palavra**, prevendo um elemento da sequÃªncia **com base nos elementos anteriores**



ğŸ“Œ **Exemplo de uma traduÃ§Ã£o InglÃªs â†’ AlemÃ£o**  

âœ… Entrada: `"The cat is sleeping"`  
âœ… O **encoder** converte isso em um vetor contÃ­nuo **z**.  
âœ… O **decoder** gera `"Die Katze schlÃ¤ft"` palavra por palavra, com base em **z**.

## O que significa "o modelo Ã© auto-regressivo" ?

Um modelo **auto-regressivo** significa que **ele gera cada palavra com base nas palavras que jÃ¡ produziu**

- O **decoder nÃ£o gera todas as palavras de uma vez**, mas **prevÃª uma de cada vez**, alimentando a prÃ³xima previsÃ£o com a palavra anterior.


ğŸ“Œ **Exemplo prÃ¡tico (modelo auto-regressivo em aÃ§Ã£o):**

1ï¸âƒ£ O modelo comeÃ§a com um **token especial** `<start>`  
2ï¸âƒ£ Gera a primeira palavra: `"Die"`  
3ï¸âƒ£ Usa `"Die"` como entrada e prevÃª a prÃ³xima palavra: `"Katze"`  
4ï¸âƒ£ Usa `"Die Katze"` e prevÃª `"schlÃ¤ft"`  
5ï¸âƒ£ Continua atÃ© gerar toda a saÃ­da ou encontrar um **token de parada** `<end>`

Isso ajuda o modelo a garantir que a saÃ­da faÃ§a sentido gramaticalmente.

- Modelos auto-regressivos permitem que a saÃ­da seja **contextual**, ou seja, a escolha de cada palavra influencia as prÃ³ximas.
- No entanto, esse processo tambÃ©m **impede paralelizaÃ§Ã£o**, porque cada palavra depende da anterior.

Por isso, **o Transformer substitui esse processo** por um **mecanismo de atenÃ§Ã£o**, permitindo processar tudo em paralelo! ğŸš€ğŸ”¥


![[Pasted image 20250214122916.png]]



## Arquitetura

![[Pasted image 20250214123003.png]]

A arquitetura do **Transformer** funciona em camadas empilhadas (N = 6) para capturar relaÃ§Ãµes complexas entre as palavras. Cada camada tem **sub-mÃ³dulos** responsÃ¡veis por processar a informaÃ§Ã£o.

### **ğŸ”· 1. Encoder (lado esquerdo da imagem)**

- O **encoder** recebe uma sequÃªncia de entrada e a transforma em **vetores de contexto** que representam a estrutura e significado da frase.
- Ele Ã© composto por **6 camadas idÃªnticas**, cada uma contendo:
    1. **Multi-Head Self-Attention** â€“ Permite que cada palavra se relacione com todas as outras palavras da entrada.
    2. **Feed-Forward Layer** â€“ Rede totalmente conectada que processa as informaÃ§Ãµes das atenÃ§Ãµes.
    3. **Residual Connection & Layer Normalization** â€“ MantÃ©m o fluxo de informaÃ§Ã£o estÃ¡vel.

ğŸ“Œ **Positional Encoding:** Como o Transformer **nÃ£o Ã© recorrente**, ele adiciona informaÃ§Ãµes de posiÃ§Ã£o para que o modelo saiba a ordem das palavras.

---

### **ğŸ”· 2. Decoder (lado direito da imagem)**

- O **decoder** recebe a saÃ­da do encoder e gera a sequÃªncia de saÃ­da palavra por palavra.
- TambÃ©m possui **6 camadas idÃªnticas**, mas com um componente extra:
    1. **Masked Multi-Head Attention** â€“ Funciona como o self-attention do encoder, mas impede que o modelo veja palavras futuras. Isso garante que a previsÃ£o para a palavra **i** sÃ³ dependa das palavras anteriores.
    2. **Multi-Head Attention** (normal) â€“ AtenÃ§Ã£o sobre a saÃ­da do encoder, permitindo que o decoder use informaÃ§Ãµes do contexto de entrada.
    3. **Feed-Forward Layer** â€“ Rede totalmente conectada que processa as informaÃ§Ãµes da atenÃ§Ã£o.
    4. **Residual Connection & Layer Normalization** â€“ Como no encoder.

ğŸ“Œ **Shifted Output (Deslocamento da saÃ­da)** â€“ Para evitar que o modelo veja a palavra certa antes de gerÃ¡-la, a saÃ­da Ã© deslocada (shifted right), ou seja, cada previsÃ£o sÃ³ vÃª palavras passadas.

---

### **ğŸ”· 3. Como funciona a geraÃ§Ã£o da saÃ­da?**

1. O modelo recebe a frase de entrada (ex.: `"The cat is sleeping"`).
2. O encoder processa e gera representaÃ§Ãµes contextuais.
3. O decoder recebe a saÃ­da deslocada (ex.: `"<start> Die Katze"`).
4. O decoder prevÃª a prÃ³xima palavra (`"schlÃ¤ft"`), sem ver as palavras futuras.
5. Esse processo continua atÃ© prever `"<end>"`, indicando o fim da frase.

ğŸ“Œ No final, um **softmax** Ã© aplicado para obter probabilidades sobre o vocabulÃ¡rio, e a palavra mais provÃ¡vel Ã© escolhida.

---

### **ğŸ” Resumo final**

âœ… O **encoder** processa a entrada usando **self-attention e redes feed-forward**.  
âœ… O **decoder** gera palavras **uma por vez**, usando **mÃ¡scaras para nÃ£o olhar o futuro**.  
âœ… O modelo usa **positional encoding** porque **nÃ£o tem recorrÃªncia**.  
âœ… O **softmax** escolhe a palavra mais provÃ¡vel em cada passo.


## Por que autoatenÃ§Ã£o ?

### Complexidade computacional

| **Camada**             | **Complexidade por camada**                          |
| ---------------------- | ---------------------------------------------------- |
| Self-Attention         | **O(nÂ²Â·d)** (depende da sequÃªncia n e da dimensÃ£o d) |
| RecorrÃªncia (RNN/LSTM) | **O(nÂ·dÂ²)** (custo cresce linearmente com n)         |
| ConvoluÃ§Ã£o (CNN)       | **O(kÂ·nÂ·d + nÂ·dÂ²)** (depende do tamanho do kernel k) |

- **Self-Attention** Ã© mais eficiente que [[RNNs]] quando n < d ( o que Ã© comum em PLN).
- **CNNs** podem ter menos complexidade, mas precisam de vÃ¡rias camadas para cobrir toda a entrada.
- Self-Attention pode ser ajustada para melhorar o desempenho em sequÃªncias longas.


### ParalelizaÃ§Ã£o

| **Camada**             | NÃºmero de operaÃ§Ãµes sequenciais                                                    |
| ---------------------- | ---------------------------------------------------------------------------------- |
| Self-Attention         | **O(1)** (todas as palavras interagem simultaneamente)                             |
| RecorrÃªncia (RNN/LSTM) | **O(n)** (processa palavra por palavra, sequencialmente)                           |
| ConvoluÃ§Ã£o (CNN)       | **O(n/k)** (usa mÃºltiplos filtros simultÃ¢neos, mas ainda precisa empilhar camadas) |

- [[RNNs]] precisam processar cada palavra sequencialmente, dificultando o paralelismo.
- [[CNNs]] podem ser mais paralelizÃ¡veis que RNNs, mas ainda precisam de mÃºltiplas camadas para capturar todo o contexto.
- Self-Attention permite computaÃ§Ã£o completamente paralela, acelerando significativamente o treinamento em GRUs e [[TPUs]]

### Captura de dependÃªncia de longo alcance

| **Camada**             | MÃ¡ximo comprimento do caminho                                                 |
| ---------------------- | ----------------------------------------------------------------------------- |
| Self-Attention         | **O(1)** (todas as palavras interagem diretamente)                            |
| RecorrÃªncia (RNN/LSTM) | **O(n)** (o sinal precisa passar por todas as palavras anteriores)            |
| ConvoluÃ§Ã£o (CNN)       | **O(n/k) ou O(logâ‚–(n))** (depende do tamanho do kernel e convoluÃ§Ã£o dilatada) |

- **Self-Attention conecta todas as palavras diretamente**,  tornando mais fÃ¡cil capturar **relaÃ§Ãµes de longo prazo**
- Em **RNNs/LSTMs**, a informaÃ§Ã£o precisa percorrer toda a sequÃªncia, o que causa dificuldade com frases longas.
- **CNNs** podem melhorar isso, mas precisam de mÃºltiplas camadas para cobrir grandes distÃ¢ncias.


![[Pasted image 20250214135341.png]]



## WMT e Treinamento

Workshop on Machine Translation, um evento anual onde pesquisadores testam seus modelos em benchmarks de traduÃ§Ã£o.

### Treinamento

Antes de treinar o modelo, as frases foram **convertidas para tokens numÃ©ricos** usando **Byte-Pair Encoding (BPE)**.

ğŸ”¹ **O que Ã© Byte-Pair Encoding (BPE)?**

- Um mÃ©todo de **tokenizaÃ§Ã£o subpalavra** que divide palavras em partes menores, ajudando o modelo a lidar com palavras desconhecidas.
- Funciona **aprendendo padrÃµes frequentes em palavras e criando tokens reutilizÃ¡veis**.

ğŸ“Œ **Exemplo de BPE:**  
A palavra **"unhappiness"** pode ser dividida em **"un", "happiness"**, e depois "happiness" pode ser dividida em **"happi" + "ness"**. Isso ajuda a lidar com palavras novas e morfologicamente complexas.

ğŸ”¹ **VocabulÃ¡rio treinado:**

- **InglÃªs-AlemÃ£o** â†’ 37.000 tokens
- **InglÃªs-FrancÃªs** â†’ 32.000 tokens

ğŸ“Œ **Por que BPE Ã© Ãºtil?**  
âœ… Resolve problemas com palavras desconhecidas (OOV - Out of Vocabulary).  
âœ… Melhora a eficiÃªncia do treinamento.  
âœ… Reduz o tamanho do vocabulÃ¡rio, tornando o modelo mais leve.

####  **OrganizaÃ§Ã£o dos Dados para Treinamento (Batching)**

ğŸ”¹ **O que Ã© um batch?**

- No treinamento de redes neurais, os dados sÃ£o processados **em lotes (batches)**, em vez de treinar com uma sentenÃ§a por vez.

ğŸ”¹ **Como os batches foram organizados?**

- As frases foram agrupadas por **tamanho aproximado** para otimizar a eficiÃªncia computacional.
- **Cada batch continha aproximadamente 25.000 tokens de origem e 25.000 tokens de destino.**

ğŸ“Œ **Por que isso Ã© importante?**  
âœ… **Batching eficiente melhora o uso da memÃ³ria da GPU.**  
âœ… **Agrupar sentenÃ§as por tamanho reduz desperdÃ­cio de computaÃ§Ã£o.**  
âœ… **Ajuda a treinar modelos mais rÃ¡pido e de forma mais estÃ¡vel.**


## RegularizaÃ§Ã£o


RegularizaÃ§Ã£o Ã© um conjunto de tÃ©cnicas utilizadas para **evitar overfitting** (quando o modelo aprende muito bem os dados de treino, mas nÃ£o generaliza bem para novos dados).


Nesta seÃ§Ã£o, o **Transformer** usa **trÃªs tipos principais de regularizaÃ§Ã£o**:

1ï¸âƒ£ **Dropout Residual**  
2ï¸âƒ£ **Label Smoothing**  
3ï¸âƒ£ **ComparaÃ§Ã£o com outros modelos em BLEU e custo computacional (FLOPs)**


### **1ï¸âƒ£ Residual Dropout ğŸ›‘**

ğŸ”¹ **O que Ã© Dropout?**

- Dropout Ã© uma tÃ©cnica que **"desliga" aleatoriamente alguns neurÃ´nios** durante o treinamento para evitar que o modelo fique muito dependente de certas conexÃµes.

ğŸ”¹ **Como Ã© aplicado no Transformer?**

- **Em cada subcamada do Transformer**, antes da normalizaÃ§Ã£o.
- **No embedding e no positional encoding** (no encoder e no decoder).
- **Taxa de dropout = 0.1 (10%)** para o modelo base.

ğŸ“Œ **Por que isso Ã© Ãºtil?**  
âœ… Evita **overfitting**, tornando o modelo mais robusto.  
âœ… Ajuda a melhorar a **generalizaÃ§Ã£o** para novos textos.


### **2ï¸âƒ£ Label Smoothing ğŸ”„**

ğŸ”¹ **O que Ã© Label Smoothing?**

- Normalmente, modelos de PLN aprendem que a palavra correta tem **100% de probabilidade** (ex.: um modelo treinado para prever `"cachorro"` pode dar 100% de certeza para essa palavra).
- **Label Smoothing suaviza isso**, atribuindo **pequena probabilidade para outras palavras similares**.

ğŸ”¹ **Como foi aplicado?**

- O Transformer usa **Ïµ_ls = 0.1**, ou seja, em vez de dar **100% de certeza para a palavra correta**, ele redistribui **10%** para palavras similares.

ğŸ“Œ **Por que isso Ã© Ãºtil?**  
âœ… Evita que o modelo fique **muito confiante e rÃ­gido** nas previsÃµes.  
âœ… Melhora **BLEU score e acurÃ¡cia** na traduÃ§Ã£o.  
âœ… Pode ajudar na **generalizaÃ§Ã£o**, tornando o modelo mais flexÃ­vel para palavras sinÃ´nimas.

ğŸ“Œ **Desvantagem**:

- Aumenta a **perplexidade** (o modelo parece menos confiante), mas melhora o desempenho geral.


### **3ï¸âƒ£ ComparaÃ§Ã£o do Transformer com outros modelos (BLEU e FLOPs)**

A tabela mostra como o Transformer supera os modelos anteriores **com menor custo computacional**.

ğŸ“Œ **MÃ©tricas importantes na tabela:**

- **BLEU Score** â†’ Mede a qualidade da traduÃ§Ã£o (quanto maior, melhor).
- **Training Cost (FLOPs)** â†’ Mede o custo computacional (quanto menor, melhor).

|**Modelo**|**BLEU (EN-DE)**|**BLEU (EN-FR)**|**Custo Computacional (FLOPs)**|
|---|---|---|---|
|ByteNet|23.75|-|-|
|GNMT + RL|24.6|39.92|2.3 Ã— 10Â¹â¹|
|ConvS2S|25.16|40.46|9.6 Ã— 10Â¹â¸|
|MoE|26.03|40.56|2.0 Ã— 10Â¹â¹|
|Transformer (Base)|**27.3**|38.1|**3.3 Ã— 10Â¹â¸**|
|Transformer (Big)|**28.4**|**41.8**|**2.3 Ã— 10Â¹â¹**|

ğŸ”¹ **Por que o Transformer se destaca?**  
âœ… **BLEU Score mais alto** â†’ Melhor qualidade de traduÃ§Ã£o.  
âœ… **Menor custo computacional** â†’ Treina mais rÃ¡pido e com menos recursos.  
âœ… **Menos tempo de treinamento** â†’ Apenas **12h no modelo base e 3,5 dias no modelo grande**.

